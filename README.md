SIGNIFY: REAL TIME SIGN LANGUAGE TO SPEECHğŸ–ï¸ğŸ”Š

This project uses AI and computer vision to detect sign language gestures in real-time and convert them into spoken words. The system leverages Mediapipe for hand tracking, a gesture classification model, and text-to-speech engines (pyttsx3 / gTTS / edge-tts) to enable seamless communication.

ğŸ“Œ Features

âœ… Real-time sign language gesture detection (ASL / custom gestures)
âœ… Gesture-to-speech conversion with AI-powered text-to-speech
âœ… Works on images, webcam video feeds, and recorded videos
âœ… Fast and accurate recognition for everyday communication
âœ… Output displays gesture name and spoken word in real-time

ğŸ› ï¸ Tech Stack

ğŸ Python 3
ğŸ¥ OpenCV â€“ real-time video capture and processing
âœ‹ Mediapipe â€“ hand landmark detection
ğŸ¤– TensorFlow / PyTorch â€“ gesture classification
ğŸ”Š pyttsx3 / gTTS / edge-tts â€“ text-to-speech conversion
â˜ï¸ Google Colab â€“ run without local setup

ğŸ“‚ Project Structure
Signify-Sign-to-Speech/
â”‚
â”œâ”€â”€ weights/
â”‚   â”œâ”€â”€ gesture_model.pt       # Trained gesture classification model
â”œâ”€â”€ main.py                    # Python script to run the application
â”œâ”€â”€ demo_image.jpg             # Sample input image for testing
â”œâ”€â”€ demo_video.mp4             # Sample video input (optional)
â”œâ”€â”€ app.ipynb                  # Google Colab notebook
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # Project documentation

2ï¸âƒ£ Install Dependencies

Make sure you are inside the project folder, then install dependencies:

cd Signify-Sign-to-Speech
pip install -r requirements.txt
